{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "19232697 ANLP Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w_gSJ9wV_lJ",
        "colab_type": "text"
      },
      "source": [
        "ANLP Assignment 1\n",
        "\n",
        "Saurabh Hebbalkar\n",
        "\n",
        "19232697"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paEhnTp2QID7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Required Imports\n",
        "%tensorflow_version 2.x \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.utils import shuffle\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Embedding\n",
        "from sklearn import svm\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import itertools\n",
        "from sklearn import metrics\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qw0tO809xaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading the File\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxYwxMA55sNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loding the file and storing in the dataframe\n",
        "data = pd.read_csv('SemEval2018-T3-train-taskA.txt', sep='\\t')\n",
        "#Renaming the columns\n",
        "data.columns = ['Tweet_index', 'Label','Tweet_text']\n",
        "\n",
        "#Storing the Tweet Text as list\n",
        "corpus = data['Tweet_text'].tolist()\n",
        "#Storing the lables as sentiments\n",
        "sentiment = data['Label']\n",
        "\n",
        "positive_example = 0\n",
        "negative_example = 0\n",
        "\n",
        "#Getting all the words in one list\n",
        "wordList =[]\n",
        "for words in corpus:\n",
        "    wordList.append(words.lower().strip().split())\n",
        "\n",
        "wordList = list(itertools.chain.from_iterable(wordList))\n",
        "#Set fetches the unique words and we add it to the vocabulary\n",
        "vocabulary = set(wordList)\n",
        "vocabulary_len = len(vocabulary)\n",
        "\n",
        "for i in range(len(sentiment)):\n",
        "  if(sentiment[i] == 1):\n",
        "    positive_example = positive_example + 1\n",
        "  else:\n",
        "    negative_example = negative_example + 1\n",
        "\n",
        "print(\"Size of the Vocabulary:\",vocabulary_len)\n",
        "print(\"Total Number of Positive Examples:\",positive_example)\n",
        "print(\"Total Number of Negative Examples:\",negative_example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA3ZDO-Ym-lA",
        "colab_type": "text"
      },
      "source": [
        "Task 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA8HTKHgauzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using the data for train test split\n",
        "x_data = data['Tweet_text']\n",
        "y_labels = data['Label']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcWlBITUhWhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to calculate precision recall and F-Measure\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "  #Accepting Actual values and Predicted values to calculate metric scores\n",
        "  precision_score = metrics.precision_score(y_true, y_pred)\n",
        "  recall_score = metrics.recall_score(y_true, y_pred)\n",
        "  f1_score = metrics.f1_score(y_true, y_pred)\n",
        "  return precision_score, recall_score, f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE19F_X6nJ3s",
        "colab_type": "text"
      },
      "source": [
        "Task 3\n",
        "\n",
        "https://towardsdatascience.com/sarcasm-detection-step-towards-sentiment-analysis-84cb013bb6db"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE6sVn7unMBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting words into tokens(numbers)\n",
        "vectorizer = CountVectorizer(ngram_range=(1,1))\n",
        "vectorizer.fit(corpus)\n",
        "#transforming output to array\n",
        "x_data = vectorizer.transform(corpus).toarray()\n",
        "#Splitting train test data\n",
        "#I am splliting 30% for test and 70% for training\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_data,y_labels,test_size=0.3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv2Sjm7_CtYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logistic Regression has the log linear model\n",
        "\n",
        "lr = LogisticRegression()\n",
        "#Training the model on train data\n",
        "lr.fit(X_train, y_train)\n",
        "#Creating list for storing the predictions\n",
        "prediction = []\n",
        "for i in X_test:\n",
        "  #predicting all the test data\n",
        "  prediction.append(lr.predict(i.reshape(1,-1)))\n",
        "\n",
        "#calling the defined metrics function to get the Precison recall and F-measure score\n",
        "print(\"Precison, Recall and F-Measure Score for Logistic Regression is:\")\n",
        "print(calculate_metrics(y_test, prediction))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NkGUO7bgKb_",
        "colab_type": "text"
      },
      "source": [
        "Task 4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Np4FBzfzX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Max Lenght of the Output Dimension\n",
        "max_len = 128\n",
        "\n",
        "#Getting the data for Test Train split\n",
        "x_data_nn = data['Tweet_text'].tolist()\n",
        "y_labels_nn = data['Label'].tolist()\n",
        "\n",
        "#Splitting the data in to test and train with 30% for test and 70% for train\n",
        "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(x_data_nn,y_labels_nn,test_size=0.3, shuffle=True)\n",
        "print(len(X_train_nn), len(X_test_nn))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RB2CrjegIUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocessing data for Keras Model\n",
        "#Converting class vector to binary class matrix.\n",
        "y_train_nn = to_categorical(y_train_nn)\n",
        "\n",
        "#Converting sentences into tokens as an input for the model\n",
        "tokenizer = Tokenizer(num_words = vocabulary_len)\n",
        "\n",
        "#Vectorizing the corpus\n",
        "#Updating internal vocabulary based on a list of texts\n",
        "tokenizer.fit_on_texts(X_train_nn)\n",
        "\n",
        "#Storing Label test data before preprocessing into another list so that it can be used later with original format\n",
        "y_test_nn1 = y_test_nn\n",
        "\n",
        "#Transforming each text in texts to a sequence of integers\n",
        "X_train_nn = tokenizer.texts_to_sequences(X_train_nn)\n",
        "\n",
        "#Padding sequences to the same length. The smaller sententces will be padded with 0 and thus model understands it\n",
        "X_train_nn = pad_sequences(X_train_nn, maxlen = max_len)\n",
        "\n",
        "#Transforming each text in texts to a sequence of integers for test data\n",
        "X_test_nn = tokenizer.texts_to_sequences(X_test_nn)\n",
        "\n",
        "#Padding sequences to the same length for test data\n",
        "X_test_nn = pad_sequences(X_test_nn, maxlen = max_len)\n",
        "y_test_nn = to_categorical(y_test_nn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptCdltUjj56W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Keras Model\n",
        "#Sequential Model\n",
        "#Embedding layer for similarity in words\n",
        "#This layer encodes the input sequence into a sequence of dense vectors.\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocabulary_len, max_len),\n",
        "])\n",
        "#Implementing LSTM layer\n",
        "#LSTM layer uses gates to control the memorizing process\n",
        "#It transforms the vector sequence into a single vector of size comtaining sequence information.\n",
        "model.add(tf.keras.layers.LSTM(100))\n",
        "\n",
        "#Dense layer with activation function as softmax\n",
        "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "#Compiling the model with binary crossentropy loss functions and stochastic gradient descent optimizer\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='sgd', metrics = ['accuracy'])\n",
        "\n",
        "#Summerizing the model\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcVjJzVkkcJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Traning the model with 10 batch size and 10 iterations\n",
        "model.fit(x =X_train_nn, y=y_train_nn ,epochs = 10, batch_size = 10,\n",
        "                    validation_data=(X_test_nn, y_test_nn)\n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HrXK4iEn4el",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the Model\n",
        "model.save('FinalModel.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_P0nIs9WPWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predicting the classes for the test data and storing in the list\n",
        "predictionRNN = model.predict_classes(X_test_nn)\n",
        "\n",
        "#Fecthing the Metric results\n",
        "print(\"Precison, Recall and F-Measure Score is:\")\n",
        "print(calculate_metrics(y_test_nn1,predictionRNN))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtnRpm-m-DC3",
        "colab_type": "text"
      },
      "source": [
        "Task 5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUw96-i_QOHj",
        "colab_type": "text"
      },
      "source": [
        "Improved Model\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "From the above observation, the (Task 4) Keras model was overfitting. The model was performing decent on train data but was performing poorly on test data. The metric score was also poor. The reason for this poor performace was the pre-processing of data and the structure of the model. The preprocessing was only converting string into lowercase and then generating tokens. The model had Stochastic Gradient Descent optimizer and did not have dropout layer.\n",
        "In the imporoved model I added  Dropout layer as it randomly ignores the nodes and thus avoides overfitting. As per the lecture 3; hinge loss works well for the binary classifcation and so I used the hing loss function in the model for better results. Adaptive optimizer \"adam\" is used instead of SGD. I also preprocessed the data thoroughly and removed the stopwords, punctuations and special characters. I also removed numbers from the data as it might create problem in tokenzing the input for the keras model.\n",
        "The model performance did not improve my much but the improvment is visible through evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c84XWbh6O0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data Enhancement \n",
        "\n",
        "data = pd.read_csv('SemEval2018-T3-train-taskA.txt', sep='\\t')\n",
        "data.columns = ['Tweet_index', 'Label','Tweet_text']\n",
        "\n",
        "#Data preprocessing and removing the unwanted data\n",
        "#Removing Numbers\n",
        "data['Tweet_text'] = data['Tweet_text'].str.replace('\\d+', '')\n",
        "#Removing -- character\n",
        "data['Tweet_text'] = data['Tweet_text'].str.replace(\"--\", '')\n",
        "#Removing Punctuations\n",
        "data['Tweet_text'] = data['Tweet_text'].str.replace('[^\\w\\s]','')\n",
        "#Lowercase and splitting sentences into tokens\n",
        "data['Tweet_text'] = data['Tweet_text'].str.lower()\n",
        "#Removing Stop words\n",
        "data['Tweet_text'].apply(lambda x: [item for item in x if item not in stop])\n",
        "\n",
        "corpus = data['Tweet_text'].tolist()\n",
        "\n",
        "wordList =[]\n",
        "for words in corpus:\n",
        "      #print(words)\n",
        "      wordList.extend(words.split())\n",
        "\n",
        "#wordList = list(itertools.chain.from_iterable(wordList))\n",
        "vocabulary = set(wordList)\n",
        "vocabulary_len = len(vocabulary)\n",
        "\n",
        "#After preprocessing the data, the updated vacabluary has less words\n",
        "print(\"Size of the Updated Vocabulary:\",vocabulary_len)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJrP2-OOACxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data_nn = data['Tweet_text'].tolist()\n",
        "y_labels = data['Label'].tolist()\n",
        "\n",
        "#Max Lenght of the Output Dimension\n",
        "max_len = 128\n",
        "\n",
        "#Getting the data for Test Train split\n",
        "x_data_nn = data['Tweet_text'].tolist()\n",
        "y_labels_nn = data['Label'].tolist()\n",
        "\n",
        "#Splitting the data in to test and train with 30% for test and 70% for train\n",
        "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(x_data_nn,y_labels_nn,test_size=0.3, shuffle=True)\n",
        "print(len(X_train_nn), len(X_test_nn))\n",
        "\n",
        "#Preprocessing data for Keras Model\n",
        "#Converting class vector to binary class matrix.\n",
        "y_train_nn = to_categorical(y_train_nn)\n",
        "\n",
        "#Converting sentences into tokens as an input for the model\n",
        "tokenizer = Tokenizer(num_words = vocabulary_len)\n",
        "\n",
        "#Vectorizing the corpus\n",
        "#Updating internal vocabulary based on a list of texts\n",
        "tokenizer.fit_on_texts(X_train_nn)\n",
        "\n",
        "#Storing Label test data before preprocessing into another list so that it can be used later with original format\n",
        "y_test_nn1 = y_test_nn\n",
        "\n",
        "#Transforming each text in texts to a sequence of integers\n",
        "X_train_nn = tokenizer.texts_to_sequences(X_train_nn)\n",
        "\n",
        "#Padding sequences to the same length. The smaller sententces will be padded with 0 and thus model understands it\n",
        "X_train_nn = pad_sequences(X_train_nn, maxlen = max_len)\n",
        "\n",
        "#Transforming each text in texts to a sequence of integers for test data\n",
        "X_test_nn = tokenizer.texts_to_sequences(X_test_nn)\n",
        "\n",
        "#Padding sequences to the same length for test data\n",
        "X_test_nn = pad_sequences(X_test_nn, maxlen = max_len)\n",
        "y_test_nn = to_categorical(y_test_nn)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aavIEtZuUGJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sequential Model with Embedding layer just like above\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocabulary_len, max_len),\n",
        "])\n",
        "#Adding two LSTM layers one returns the sequence other doesnt\n",
        "model.add(tf.keras.layers.LSTM(100, return_sequences=True))\n",
        "model.add(tf.keras.layers.LSTM(100, return_sequences=False))\n",
        "#Drouput layer is used to avoid overfitting. Randomly ignores the nodes from the layer and thus overfitting is avoided\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "#Activation function used is softmax\n",
        "model.add(tf.keras.layers.Dense(2, activation='relu'))\n",
        "\n",
        "#Using Hinge loss as it is better for binary classification (From lecture 3 slides) & Adaptive Adam optimizer\n",
        "model.compile(loss = 'squared_hinge', optimizer=tf.keras.optimizers.Adam(1e-4), metrics = ['accuracy'])\n",
        "\n",
        "#Summary of Model\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPaUrxO-k-3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model Training\n",
        "model.fit(x =X_train_nn, y=y_train_nn ,epochs = 10, batch_size = 10,\n",
        "                    validation_data=(X_test_nn, y_test_nn)\n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcpZmKnyyLme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENpredictionRNN = model.predict_classes(X_test_nn)\n",
        "\n",
        "print(calculate_metrics(y_test_nn1,ENpredictionRNN))\n",
        "#predictionsvm.append(svclassifier.predict(i.reshape(1,-1)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}